{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from pandas.plotting import scatter_matrix\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your functions from fetch_data.py\n",
    "from fetch_data import fetch_housing_data, load_housing_data\n",
    "# Download and extract data if not done already\n",
    "fetch_housing_data()\n",
    "# Load the housing data into a pandas DataFrame\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç **Take a Quick Look at the Data Structure**\n",
    "\n",
    "Let‚Äôs begin by examining the top five rows using the DataFrame‚Äôs `head()` method.  \n",
    "Each row in the dataset represents a district in California.\n",
    "\n",
    "\n",
    "#### üìå **There are 10 attributes in the dataset:**\n",
    "\n",
    "- `longitude`  \n",
    "- `latitude`  \n",
    "- `housing_median_age`  \n",
    "- `total_rooms`  \n",
    "- `total_bedrooms`  \n",
    "- `population`  \n",
    "- `households`  \n",
    "- `median_income`  \n",
    "- `median_house_value`  \n",
    "- `ocean_proximity`  \n",
    "\n",
    "\n",
    "The `info()` method provides a quick summary of the dataset, including:\n",
    "- The total number of entries (rows)\n",
    "- Data types of each column\n",
    "- The number of non-nul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# only in a Jupyter notebook\n",
    "\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice a few things in these histograms:\n",
    "\n",
    "1. **Median Income Scale**  \n",
    "   The `median_income` attribute does not appear to be expressed in US dollars (USD).  \n",
    "   After checking with the data collection team, you learn that:\n",
    "   - The data has been **scaled** and **capped** at **15.0001** for high incomes and at **0.4999** for low incomes.\n",
    "   - The values represent **tens of thousands of dollars** (e.g., `3` ‚âà `$30,000`).\n",
    "\n",
    "   > üìå Working with preprocessed attributes is common in Machine Learning.  \n",
    "   > It's not always an issue, but understanding how the data was processed is important.\n",
    "\n",
    "2. **Capped Values: Housing Median Age and Median House Value**  \n",
    "   - These features were also **capped**.\n",
    "   - Capping the **median house value** may be a serious issue since it is the **target attribute** (label).\n",
    "   - If the ML algorithm sees only values below a certain threshold (e.g., \\$500,000), it may **learn incorrectly** that values never exceed this.\n",
    "\n",
    "   #### What to do if precise predictions > \\$500,000 are needed?\n",
    "   - **Option 1**: Collect **uncapped labels** for those districts.\n",
    "   - **Option 2**: Remove those districts from the **training and test sets** so the model is not unfairly penalized.\n",
    "\n",
    "3. **Different Feature Scales**  \n",
    "   - The features have very **different scales** (e.g., income vs. housing age).\n",
    "   - This will require **feature scaling**, which we'll explore later in this chapter.\n",
    "\n",
    "4. **Tail-Heavy Distributions**  \n",
    "   - Many histograms are **right-skewed** (tail heavy).\n",
    "   - This makes it harder for some ML algorithms to **detect patterns**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ **Create a Test Set**\n",
    "\n",
    "It may seem strange to set aside part of the data before diving deeper into analysis.  \n",
    "After all, shouldn‚Äôt we learn more about the data first?\n",
    "\n",
    "However:\n",
    "\n",
    "> ‚ö†Ô∏è **Your brain is an amazing pattern detection system, and that makes it prone to overfitting.**\n",
    "\n",
    "If you examine the test set early, you may notice patterns and **subconsciously overfit** your model.  \n",
    "This leads to **data snooping bias**: your system performs well in testing, but poorly in the real world.\n",
    "\n",
    "#### ‚úÖ **Best Practice:**\n",
    "\n",
    "Create a test set **right away**:\n",
    "\n",
    "- Pick a random subset of the data ‚Äî typically **20%** (or less if the dataset is large)\n",
    "- Set it aside and **never look at it** until the final evaluation stage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_data(data , test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices] , data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set , test_set = split_train_data(housing , 0.2)\n",
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ **Reliable & Representative Test Set Creation , Stratified Sampling**\n",
    "\n",
    "Splitting the dataset randomly can cause the test set to have skewed proportions of key categories. Stratified sampling divides the data into subgroups (strata) and samples proportionally from each, preserving the overall distribution in the test set.\n",
    "\n",
    "#### üéØ **Why Stratified Sampling?**\n",
    "\n",
    "Purely random sampling usually works well with large datasets. However, if the dataset is small or some features are unevenly distributed, random sampling may produce **biased samples**.\n",
    "\n",
    "For example, in a survey of 1,000 people, the US population is roughly 51.3% female and 48.7% male. A well-designed survey maintains this ratio to get **representative results**. Pure random sampling could result in skewed gender ratios in the sample, biasing the outcomes.\n",
    "\n",
    "This approach of dividing the population into **homogeneous subgroups (strata)** and sampling proportionally from each is called **stratified sampling**. It ensures the test set reflects the overall population accurately.\n",
    "\n",
    "\n",
    "#### üè∑Ô∏è **Creating Income Categories for Stratification**\n",
    "\n",
    "If a numerical feature like **median income** is crucial for prediction, you can turn it into a categorical attribute to stratify by income level.\n",
    "\n",
    "Using `pd.cut()`, the continuous median income is split into 5 categories representing income ranges:\n",
    "\n",
    "- Category 1: 0 to 1.5 (less than $15,000)  \n",
    "- Category 2: 1.5 to 3  \n",
    "- Category 3: 3 to 4.5  \n",
    "- Category 4: 4.5 to 6  \n",
    "- Category 5: 6 and above\n",
    "\n",
    "This helps ensure the train and test sets maintain similar distributions of income levels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß™ **Train-Test Split with Stratified Sampling**\n",
    "\n",
    "To ensure the test set represents the overall population well‚Äîespecially for imbalanced categories‚Äîwe used **Stratified Sampling** via `Scikit-Learn`.\n",
    "\n",
    "- The `ocean_proximity` feature is categorical and location-based, making it a good candidate for stratified sampling.\n",
    "- To stratify on income, we created an `income_cat` attribute using the `median_income` feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['income_cat'] = pd.cut(housing['median_income'] , bins=[0.,1.5,3.0,4.5,6. , np.inf] , labels=[1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['income_cat'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1 , test_size=0.2 , random_state=42)\n",
    "for train_index , test_index in split.split(housing , housing['income_cat']):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let‚Äôs see if this worked as expected. You can start by looking at the income category\n",
    "proportions in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set.income_cat.value_counts()/len(strat_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We then performed a **Stratified Shuffle Split** to maintain the `income_cat` distribution in both training and test sets.\n",
    "\n",
    "After the split, we removed the `income_cat` column to restore the datasets to their original state:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Next Step: Data Exploration\n",
    "\n",
    "With a properly split dataset, we now begin **exploring the data** to gain insights and prepare it for the machine learning pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìç **Visualizing Geographical Data**\n",
    "\n",
    "We have **geographical information**‚Äîspecifically `latitude` and `longitude`. Plotting these as a scatterplot helps visualize **district locations** on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåç **Visualizing Geographical Data with Housing Prices**\n",
    "\n",
    "Now that we have a basic scatter plot of the districts using longitude and latitude, we can improve the visualization by adding more dimensions to the plot:\n",
    "\n",
    "- **Circle radius** will represent the population of each district.\n",
    "- **Color** will represent the median house value.\n",
    "- We'll use the `jet` colormap (`cmap='jet'`) which ranges from blue (low) to red (high prices).\n",
    "\n",
    "This enhanced scatter plot helps us spot geographic patterns in housing prices and population distribution more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(\n",
    "    kind=\"scatter\", \n",
    "    x=\"longitude\", \n",
    "    y=\"latitude\", \n",
    "    alpha=0.4,\n",
    "    s=housing[\"population\"] / 100,  # radius of circle\n",
    "    label=\"Population\",\n",
    "    c=\"median_house_value\",         # color represents house value\n",
    "    cmap=plt.get_cmap(\"jet\"),       # color map\n",
    "    colorbar=True,\n",
    "    figsize=(10,7)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üó∫Ô∏è **Geographical Data Insights**\n",
    "\n",
    "This image tells you that the housing prices are very much related to the **location** (e.g., close to the ocean) and to the **population density**, as you probably knew already.\n",
    "\n",
    "It will probably be useful to:\n",
    "\n",
    "- üß† Use a **clustering algorithm** to detect the main clusters.\n",
    "- ‚ûï Add new features that measure the **proximity to the cluster centers**.\n",
    "- üåä Consider the **ocean proximity** attribute, although in Northern California the housing prices in coastal districts are not always high ‚Äî so it's not a simple rule.\n",
    "\n",
    "These insights can guide better **feature engineering** and help improve your model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract latitude and longitude\n",
    "housing_geo = housing[[\"latitude\", \"longitude\"]]\n",
    "\n",
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "housing[\"cluster\"] = kmeans.fit_predict(housing_geo)\n",
    "\n",
    "# Add distance to cluster center as a new feature\n",
    "housing[\"dist_to_center\"] = np.linalg.norm(housing_geo - kmeans.cluster_centers_[housing[\"cluster\"]], axis=1)\n",
    "\n",
    "# Plot to visualize the clusters\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(housing[\"longitude\"], housing[\"latitude\"], c=housing[\"cluster\"], cmap='rainbow', alpha=0.6, s=10)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 1], kmeans.cluster_centers_[:, 0], c='black', s=100, label=\"Centers\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"KMeans Clustering of California Housing Districts\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç **Looking for Correlations**\n",
    "\n",
    "Since the dataset is not too large, we can compute the standard correlation coefficient (also called **Pearson‚Äôs r**) between every pair of attributes using the `corr()` method.\n",
    "\n",
    "This allows us to see how strongly each feature is linearly related to the target variable `median_house_value`.\n",
    "\n",
    "We will use the `.corr()` method on the DataFrame to get a correlation matrix and then inspect how each feature correlates with the target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr_matrix = housing.corr(numeric_only=True)\n",
    "\n",
    "# Display correlation of each feature with median_house_value\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Understanding Correlation Coefficients**\n",
    "\n",
    "The correlation coefficient ranges from **-1 to 1**:\n",
    "\n",
    "- When it is close to **1**, it indicates a **strong positive correlation**. For example, the median house value tends to increase as median income increases.\n",
    "- When it is close to **-1**, it indicates a **strong negative correlation**. For instance, there is a slight negative correlation between latitude and median house value, meaning prices tend to decrease as you move north.\n",
    "- When it is close to **0**, it means **no linear correlation** exists between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Checking Correlations with Scatter Matrix**\n",
    "\n",
    "Another useful way to explore correlations between attributes is to use Pandas' `scatter_matrix` function. This plots every numerical attribute against every other numerical attribute, making it easy to spot relationships.\n",
    "\n",
    "Since our dataset has 11 numerical attributes (which would result in 121 plots), we will focus only on a few key attributes that appear most correlated with `median_house_value` for clarity.\n",
    "\n",
    "These attributes are:  \n",
    "- `median_house_value`  \n",
    "- `median_income`  \n",
    "- `total_rooms`  \n",
    "- `housing_median_age`  \n",
    "\n",
    "Below is the scatter matrix for these attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "\"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes] , figsize=(20,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scatterplot: Median Income vs. Median House Value**\n",
    "\n",
    "The main diagonal in the scatter matrix shows histograms of each attribute, which helps us understand their distributions.\n",
    "\n",
    "Focusing on the most promising predictor ‚Äî **median income** ‚Äî we plot it against the **median house value**.\n",
    "\n",
    "This scatterplot shows:\n",
    "\n",
    "- A strong positive correlation: as median income increases, so does the median house value.\n",
    "- Visible horizontal lines indicating price caps, notably at \\$500,000 and other price thresholds around \\$450,000, \\$350,000, and \\$280,000.\n",
    "- These capped values represent quirks in the data that might mislead the model, so you may consider removing those districts during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experimenting with Attribute Combinations**\n",
    "\n",
    "Hopefully, the previous sections gave you an idea of a few ways you can explore the data and gain insights. You identified a few data quirks that you may want to clean up before feeding the data to a Machine Learning algorithm, and you found interesting correlations between attributes, in particular with the target attribute.\n",
    "\n",
    "You also noticed that some attributes have a tail-heavy distribution, so you may want to transform them (e.g., by computing their logarithm). Of course, your mileage will vary considerably with each project, but the general ideas are similar.\n",
    "\n",
    "One last thing you may want to do before actually preparing the data for Machine Learning algorithms is to try out various attribute combinations. For example, the total number of rooms in a district is not very useful if you don‚Äôt know how many households there are. What you really want is the number of rooms per household.\n",
    "\n",
    "Similarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like a useful attribute to consider.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['rooms_per_household'] = housing['total_rooms'] / housing['households']\n",
    "housing['rooms_per_household'] = housing['rooms_per_household'].round().astype(int)\n",
    "housing['total_rooms'] = housing['total_rooms'].round().astype(int)\n",
    "#housing['total_bedrooms'] = housing['total_bedrooms'].round().astype(int)\n",
    "housing['population'] = housing['population'].round().astype(int)\n",
    "housing['households'] = housing['households'].round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.select_dtypes(include=[float, int]).corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_encoded = pd.get_dummies(housing)\n",
    "corr_matrix = housing_encoded.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preparing the Data for Machine Learning Algorithms**\n",
    "\n",
    "This round of exploration does not have to be absolutely thorough; the point is to start off on the right foot and quickly gain insights that will help you get a first reasonably good prototype. \n",
    "\n",
    "This is an **iterative process**: once you get a prototype up and running, you can analyze its output to gain more insights and come back to this exploration step.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why Write Data Preparation Functions?**\n",
    "\n",
    "Instead of preparing the data manually, it‚Äôs better to write **functions** for your transformations. This has several benefits:\n",
    "\n",
    "- **Reproducibility**: Easily apply the same transformations to any dataset (e.g., when you get fresh data).\n",
    "- **Reusability**: Build a library of transformation functions for future projects.\n",
    "- **Integration**: Use the same functions in your live system to transform new data before prediction.\n",
    "- **Experimentation**: Quickly try different transformation combinations to find the best setup.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Next Step**\n",
    "\n",
    "1. Revert to a clean training set (by copying `strat_train_set` again).\n",
    "2. Separate **predictors** and **labels**, since transformations applied to features might not be the same as those applied to the target variable.\n",
    "\n",
    "> **Note:** `drop()` creates a copy of the DataFrame and does **not** modify `strat_train_set` in place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Cleaning**\n",
    "\n",
    "Most Machine Learning algorithms cannot work with missing features, so we need to handle them before training.\n",
    "\n",
    "Earlier, we noticed that the **`total_bedrooms`** attribute has some missing values. There are three common strategies to fix this:\n",
    "\n",
    "1. **Remove the corresponding rows** (districts) that have missing values.  \n",
    "2. **Remove the entire column** that contains missing values.  \n",
    "3. **Fill the missing values** with a specific value (e.g., 0, the mean, the median, etc.).\n",
    "\n",
    "\n",
    "\n",
    "#### **Pandas Methods for Handling Missing Data**\n",
    "\n",
    "- **`dropna()`** ‚Üí Remove rows with missing values.  \n",
    "- **`drop()`** ‚Üí Remove specific columns.  \n",
    "- **`fillna()`** ‚Üí Replace missing values with a given constant or a computed value.\n",
    "\n",
    "\n",
    "\n",
    "> The right choice depends on the dataset and problem ‚Äî removing too much data can harm your model, while filling with the wrong value can introduce bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Handling Missing Values with Scikit-Learn**\n",
    "\n",
    "If you choose **Option 3** (filling missing values with the median), remember:\n",
    "\n",
    "- **Compute the median** only on the **training set**.\n",
    "- **Save** the computed median so you can:\n",
    "  - Fill missing values in the test set when evaluating.\n",
    "  - Fill missing values in new incoming data once the system is live.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Using `SimpleImputer` in Scikit-Learn**\n",
    "\n",
    "Instead of manually computing and applying the median, Scikit-Learn provides the **`SimpleImputer`** class, which can automatically handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='median')\n",
    "#Since the median can only be computed on numerical attributes, we need to create a copy of the data without the text attribute ocean_proximity:\n",
    "housing_num = housing.drop('ocean_proximity' , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now you can fit the imputer instance to the training data using the fit() method:\n",
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SimpleImputer` stores each attribute‚Äôs median in `statistics_`.  \n",
    "Right now, only **`total_bedrooms`** has missing values, but future data may have gaps in other numeric fields.  \n",
    "To be safe, apply the imputer to **all numeric attributes**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the **trained** imputer to replace missing values with the learned medians:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a NumPy array of transformed features.  \n",
    "To convert it back to a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Handling Text & Categorical Values.**\n",
    "Most Machine Learning algorithms prefer to work with numbers anyway, so let‚Äôs con‚Äê\n",
    "vert these categories from text to numbers. For this, we can use Scikit-Learn‚Äôs Ordina\n",
    "lEncoder class19:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Custom Transformers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "            bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the transformer has one hyperparameter, add_bedrooms_per_room,\n",
    "set to True by default (it is often helpful to provide sensible defaults). This hyperpara‚Äê\n",
    "meter will allow you to easily find out whether adding this attribute helps the\n",
    "Machine Learning algorithms or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Scaling**\n",
    "One of the most important transformations you need to apply to your data is feature\n",
    "scaling. With few exceptions, Machine Learning algorithms don‚Äôt perform well when\n",
    "the input numerical attributes have very different scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transformation Pipelines**\n",
    "As you can see, there are many data transformation steps that need to be executed in\n",
    "the right order. Fortunately, Scikit-Learn provides the Pipeline class to help with\n",
    "such sequences of transformations. Here is a small pipeline for the numerical\n",
    "attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have handled the categorical columns and the numerical columns sepa‚Äê\n",
    "rately. It would be more convenient to have a single transformer able to handle all col‚Äê\n",
    "umns, applying the appropriate transformations to each column. In version 0.20,\n",
    "Scikit-Learn introduced the ColumnTransformer for this purpose, and the good news\n",
    "is that it works great with Pandas DataFrames. Let‚Äôs use it to apply all the transforma‚Äê\n",
    "tions to the housing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "num_attrbs = list(housing_num)\n",
    "cat_attrbs = ['ocean_proximity']\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('nums' , num_pipeline , num_attrbs),\n",
    "    ('cat' , OneHotEncoder() , cat_attrbs)\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "housing_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Select and Train a Model**\n",
    "At last! You framed the problem, you got the data and explored it, you sampled a\n",
    "training set and a test set, and you wrote transformation pipelines to clean up and\n",
    "prepare your data for Machine Learning algorithms automatically. You are now ready\n",
    "to select and train a Machine Learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training and Evaluating on the Training Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared , housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "print('Predictions: ' , lin_reg.predict(some_data_prepared))\n",
    "print('Labels:' , some_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works, although the predictions are not exactly accurate (e.g., the first prediction is\n",
    "off by close to 40%!). Let‚Äôs measure this regression model‚Äôs RMSE on the whole train‚Äê\n",
    "ing set using Scikit-Learn‚Äôs mean_squared_error function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print(f'Error: {lin_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Okay, this is better than nothing but clearly not a great score: most districts‚Äô\n",
    "median_housing_values range between $120,000 and $265,000, so a typical predic‚Äê\n",
    "tion error of $68,628 is not very satisfying. This is an example of a model underfitting\n",
    "the training data. When this happens it can mean that the features do not provide\n",
    "enough information to make good predictions, or that the model is not powerful\n",
    "enough. As we saw in the previous chapter, the main ways to fix underfitting are to\n",
    "select a more powerful model, to feed the training algorithm with better features, or\n",
    "to reduce the constraints on the model. This model is not regularized, so this rules\n",
    "out the last option. You could try to add more features (e.g., the log of the popula‚Äê\n",
    "tion), but first let‚Äôs try a more complex model to see how it does.\n",
    "Let‚Äôs train a DecisionTreeRegressor. \n",
    "\n",
    "- This is a powerful model, capable of finding\n",
    "complex nonlinear relationships in the data (Decision Trees are presented in more\n",
    "detail in Chapter 6). The code should look familiar by now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "#Now that the model is trained, let‚Äôs evaluate it on the training set:\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "print(f\"Error: {tree_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could this model really be absolutely perfect? Of course,\n",
    "it is much more likely that the model has badly overfit the data. How can you be sure?\n",
    "As we saw earlier, you don‚Äôt want to touch the test set until you are ready to launch a\n",
    "model you are confident about, so you need to use part of the training set for train‚Äê\n",
    "ing, and part for model validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
